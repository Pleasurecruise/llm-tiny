["Providedproperattributionisprovided,Googleherebygrantspermissionto\nreproducethetablesandfiguresinthispapersolelyforuseinjournalisticor\nscholarlyworks.\nAttentionIsAllYouNeed\nAshishVaswani∗\nGoogleBrain\navaswani@google.comNoamShazeer∗\nGoogleBrain\nnoam@google.comNikiParmar∗\nGoogleResearch\nnikip@google.comJakobUszkoreit∗\nGoogleResearch\nusz@google.com\nLlionJones∗\nGoogleResearch\nllion@google.comAidanN.Gomez∗†\nUniversityofToronto\naidan@cs.toronto.eduŁukaszKaiser∗\nGoogleBrain\nlukaszkaiser@google.com\nIlliaPolosukhin∗‡\nillia.polosukhin@gmail.com\nAbstract\nThedominantsequencetransductionmodelsarebasedoncomplexrecurrentor\nconvolutionalneuralnetworksthatincludeanencoderandadecoder.Thebest\nperformingmodelsalsoconnecttheencoderanddecoderthroughanattention\nmechanism.Weproposeanewsimplenetworkarchitecture,theTransformer,\nbasedsolelyonattentionmechanisms,dispensingwithrecurrenceandconvolutions\nentirely.Experimentsontwomachinetranslationtasksshowthesemodelsto\nbesuperiorinqualitywhilebeingmoreparallelizableandrequiringsignificantly\nlesstimetotrain.Ourmodelachieves28.4BLEUontheWMT2014English-\nto-Germantranslationtask,improvingovertheexistingbestresults,including\nensembles,byover2BLEU.OntheWMT2014English-to-Frenchtranslationtask,\nourmodelestablishesanewsingle-modelstate-of-the-artBLEUscoreof41.8after\ntrainingfor3.5daysoneightGPUs,asmallfractionofthetrainingcostsofthe\nbestmodelsfromtheliterature.WeshowthattheTransformergeneralizeswellto\n", "of41.8after\ntrainingfor3.5daysoneightGPUs,asmallfractionofthetrainingcostsofthe\nbestmodelsfromtheliterature.WeshowthattheTransformergeneralizeswellto\nothertasksbyapplyingitsuccessfullytoEnglishconstituencyparsingbothwithlargeandlimitedtrainingdata.\n∗Equalcontribution.Listingorderisrandom.JakobproposedreplacingRNNswithself-attentionandstarted\ntheefforttoevaluatethisidea.Ashish,withIllia,designedandimplementedthefirstTransformermodelsand\nhasbeencruciallyinvolvedineveryaspectofthiswork.Noamproposedscaleddot-productattention,multi-head\nattentionandtheparameter-freepositionrepresentationandbecametheotherpersoninvolvedinnearlyevery\ndetail.Nikidesigned,implemented,tunedandevaluatedcountlessmodelvariantsinouroriginalcodebaseand\ntensor2tensor.Llionalsoexperimentedwithnovelmodelvariants,wasresponsibleforourinitialcodebase,and\nefficientinferenceandvisualizations.LukaszandAidanspentcountlesslongdaysdesigningvariouspartsofand\nimplementingtensor2tensor,replacingourearliercodebase,greatlyimprovingresultsandmassivelyaccelerating\nourresearch.\n†WorkperformedwhileatGoogleBrain.\n‡WorkperformedwhileatGoogleResearch.\n31stConferenceonNeuralInformationProcessingSystems(NIPS2017),LongBeach,CA,USA.arXiv:1706.03762v7[cs.CL]2Aug20231Introduction\n", "edwhileatGoogleResearch.\n31stConferenceonNeuralInformationProcessingSystems(NIPS2017),LongBeach,CA,USA.arXiv:1706.03762v7[cs.CL]2Aug20231Introduction\nRecurrentneuralnetworks,longshort-termmemory[13]andgatedrecurrent[7]neuralnetworksinparticular,havebeenfirmlyestablishedasstateoftheartapproachesinsequencemodelingand\ntransductionproblemssuchaslanguagemodelingandmachinetranslation[35,2,5].Numerous\neffortshavesincecontinuedtopushtheboundariesofrecurrentlanguagemodelsandencoder-decoder\narchitectures[38,24,15].\nRecurrentmodelstypicallyfactorcomputationalongthesymbolpositionsoftheinputandoutput\nsequences.Aligningthepositionstostepsincomputationtime,theygenerateasequenceofhidden\nstatesht,asafunctionoftheprevioushiddenstateht−1andtheinputforpositiont.Thisinherently\nsequentialnatureprecludesparallelizationwithintrainingexamples,whichbecomescriticalatlonger\nsequencelengths,asmemoryconstraintslimitbatchingacrossexamples.Recentworkhasachieved\nsignificantimprovementsincomputationalefficiencythroughfactorizationtricks[21]andconditional\ncomputation[32],whilealsoimprovingmodelperformanceincaseofthelatter.Thefundamental\nconstraintofsequentialcomputation,however,remains.\nAttentionmechanismshavebecomeanintegralpartofcompellingsequencemodelingandtransduc-\n", "Thefundamental\nconstraintofsequentialcomputation,however,remains.\nAttentionmechanismshavebecomeanintegralpartofcompellingsequencemodelingandtransduc-\ntionmodelsinvarioustasks,allowingmodelingofdependencieswithoutregardtotheirdistanceintheinputoroutputsequences[2,19].Inallbutafewcases[27],however,suchattentionmechanisms\nareusedinconjunctionwitharecurrentnetwork.\nInthisworkweproposetheTransformer,amodelarchitectureeschewingrecurrenceandinstead\nrelyingentirelyonanattentionmechanismtodrawglobaldependenciesbetweeninputandoutput.\nTheTransformerallowsforsignificantlymoreparallelizationandcanreachanewstateoftheartin\ntranslationqualityafterbeingtrainedforaslittleastwelvehoursoneightP100GPUs.\n2Background\nThegoalofreducingsequentialcomputationalsoformsthefoundationoftheExtendedNeuralGPU\n[16],ByteNet[18]andConvS2S[9],allofwhichuseconvolutionalneuralnetworksasbasicbuilding\nblock,computinghiddenrepresentationsinparallelforallinputandoutputpositions.Inthesemodels,\nthenumberofoperationsrequiredtorelatesignalsfromtwoarbitraryinputoroutputpositionsgrows\ninthedistancebetweenpositions,linearlyforConvS2SandlogarithmicallyforByteNet.Thismakes\nitmoredifficulttolearndependenciesbetweendistantpositions[12].IntheTransformerthisis\n", "sitions,linearlyforConvS2SandlogarithmicallyforByteNet.Thismakes\nitmoredifficulttolearndependenciesbetweendistantpositions[12].IntheTransformerthisis\nreducedtoaconstantnumberofoperations,albeitatthecostofreducedeffectiveresolutionduetoaveragingattention-weightedpositions,aneffectwecounteractwithMulti-HeadAttentionas\ndescribedinsection3.2.\nSelf-attention,sometimescalledintra-attentionisanattentionmechanismrelatingdifferentpositions\nofasinglesequenceinordertocomputearepresentationofthesequence.Self-attentionhasbeen\nusedsuccessfullyinavarietyoftasksincludingreadingcomprehension,abstractivesummarization,\ntextualentailmentandlearningtask-independentsentencerepresentations[4,27,28,22].\nEnd-to-endmemorynetworksarebasedonarecurrentattentionmechanisminsteadofsequence-\nalignedrecurrenceandhavebeenshowntoperformwellonsimple-languagequestionansweringand\nlanguagemodelingtasks[34].\nTothebestofourknowledge,however,theTransformeristhefirsttransductionmodelrelying\nentirelyonself-attentiontocomputerepresentationsofitsinputandoutputwithoutusingsequence-\nalignedRNNsorconvolution.Inthefollowingsections,wewilldescribetheTransformer,motivate\nself-attentionanddiscussitsadvantagesovermodelssuchas[17,18]and[9].\n3ModelArchitecture\n", "n.Inthefollowingsections,wewilldescribetheTransformer,motivate\nself-attentionanddiscussitsadvantagesovermodelssuchas[17,18]and[9].\n3ModelArchitecture\nMostcompetitiveneuralsequencetransductionmodelshaveanencoder-decoderstructure[5,2,35].Here,theencodermapsaninputsequenceofsymbolrepresentations(x1,...,xn)toasequence\nofcontinuousrepresentationsz=(z1,...,zn).Givenz,thedecoderthengeneratesanoutput\nsequence(y1,...,ym)ofsymbolsoneelementatatime.Ateachstepthemodelisauto-regressive\n[10],consumingthepreviouslygeneratedsymbolsasadditionalinputwhengeneratingthenext.\n2Figure1:TheTransformer-modelarchitecture.\nTheTransformerfollowsthisoverallarchitectureusingstackedself-attentionandpoint-wise,fully\nconnectedlayersforboththeencoderanddecoder,shownintheleftandrighthalvesofFigure1,\nrespectively.\n3.1EncoderandDecoderStacks\nEncoder:TheencoderiscomposedofastackofN=6identicallayers.Eachlayerhastwo\nsub-layers.Thefirstisamulti-headself-attentionmechanism,andthesecondisasimple,position-\nwisefullyconnectedfeed-forwardnetwork.Weemployaresidualconnection[11]aroundeachof\nthetwosub-layers,followedbylayernormalization[1].Thatis,theoutputofeachsub-layeris\nLayerNorm(x+Sublayer(x)),whereSublayer(x)isthefunctionimplementedbythesub-layer\n", "ers,followedbylayernormalization[1].Thatis,theoutputofeachsub-layeris\nLayerNorm(x+Sublayer(x)),whereSublayer(x)isthefunctionimplementedbythesub-layer\nitself.Tofacilitatetheseresidualconnections,allsub-layersinthemodel,aswellastheembeddinglayers,produceoutputsofdimensiondmodel=512.\nDecoder:ThedecoderisalsocomposedofastackofN=6identicallayers.Inadditiontothetwo\nsub-layersineachencoderlayer,thedecoderinsertsathirdsub-layer,whichperformsmulti-head\nattentionovertheoutputoftheencoderstack.Similartotheencoder,weemployresidualconnections\naroundeachofthesub-layers,followedbylayernormalization.Wealsomodifytheself-attention\nsub-layerinthedecoderstacktopreventpositionsfromattendingtosubsequentpositions.This\nmasking,combinedwithfactthattheoutputembeddingsareoffsetbyoneposition,ensuresthatthe\npredictionsforpositionicandependonlyontheknownoutputsatpositionslessthani.\n3.2Attention\nAnattentionfunctioncanbedescribedasmappingaqueryandasetofkey-valuepairstoanoutput,\nwherethequery,keys,values,andoutputareallvectors.Theoutputiscomputedasaweightedsum\n3ScaledDot-ProductAttention\nMulti-HeadAttention\nFigure2:(left)ScaledDot-ProductAttention.(right)Multi-HeadAttentionconsistsofseveral\nattentionlayersrunninginparallel.\n", "tAttention\nMulti-HeadAttention\nFigure2:(left)ScaledDot-ProductAttention.(right)Multi-HeadAttentionconsistsofseveral\nattentionlayersrunninginparallel.\nofthevalues,wheretheweightassignedtoeachvalueiscomputedbyacompatibilityfunctionofthequerywiththecorrespondingkey.\n3.2.1ScaledDot-ProductAttention\nWecallourparticularattention\"ScaledDot-ProductAttention\"(Figure2).Theinputconsistsof\nqueriesandkeysofdimensiondk,andvaluesofdimensiondv.Wecomputethedotproductsofthe\nquerywithallkeys,divideeachby√dk,andapplyasoftmaxfunctiontoobtaintheweightsonthe\nvalues.\nInpractice,wecomputetheattentionfunctiononasetofqueriessimultaneously,packedtogether\nintoamatrixQ.ThekeysandvaluesarealsopackedtogetherintomatricesKandV.Wecompute\nthematrixofoutputsas:\nAttention(Q,K,V)=softmax(QKT\n√dk)V(1)\nThetwomostcommonlyusedattentionfunctionsareadditiveattention[2],anddot-product(multi-\nplicative)attention.Dot-productattentionisidenticaltoouralgorithm,exceptforthescalingfactor\nof1√dk.Additiveattentioncomputesthecompatibilityfunctionusingafeed-forwardnetworkwith\nasinglehiddenlayer.Whilethetwoaresimilarintheoreticalcomplexity,dot-productattentionis\nmuchfasterandmorespace-efficientinpractice,sinceitcanbeimplementedusinghighlyoptimized\nmatrixmultiplicationcode.\n", "calcomplexity,dot-productattentionis\nmuchfasterandmorespace-efficientinpractice,sinceitcanbeimplementedusinghighlyoptimized\nmatrixmultiplicationcode.\nWhileforsmallvaluesofdkthetwomechanismsperformsimilarly,additiveattentionoutperformsdotproductattentionwithoutscalingforlargervaluesofdk[3].Wesuspectthatforlargevaluesof\ndk,thedotproductsgrowlargeinmagnitude,pushingthesoftmaxfunctionintoregionswhereithas\nextremelysmallgradients4.Tocounteractthiseffect,wescalethedotproductsby1√dk.\n3.2.2Multi-HeadAttention\nInsteadofperformingasingleattentionfunctionwithdmodel-dimensionalkeys,valuesandqueries,\nwefounditbeneficialtolinearlyprojectthequeries,keysandvalueshtimeswithdifferent,learned\nlinearprojectionstodk,dkanddvdimensions,respectively.Oneachoftheseprojectedversionsof\nqueries,keysandvalueswethenperformtheattentionfunctioninparallel,yieldingdv-dimensional\n4Toillustratewhythedotproductsgetlarge,assumethatthecomponentsofqandkareindependentrandom\nvariableswithmean0andvariance1.Thentheirdotproduct,q·k=Pdk\ni=1qiki,hasmean0andvariancedk.\n4outputvalues.Theseareconcatenatedandonceagainprojected,resultinginthefinalvalues,as\ndepictedinFigure2.\n", "roduct,q·k=Pdk\ni=1qiki,hasmean0andvariancedk.\n4outputvalues.Theseareconcatenatedandonceagainprojected,resultinginthefinalvalues,as\ndepictedinFigure2.\nMulti-headattentionallowsthemodeltojointlyattendtoinformationfromdifferentrepresentationsubspacesatdifferentpositions.Withasingleattentionhead,averaginginhibitsthis.\nMultiHead(Q,K,V)=Concat(head1,...,headh)WO\nwhereheadi=Attention(QWQ\ni,KWK\ni,VWV\ni)\nWheretheprojectionsareparametermatricesWQ\ni∈Rdmodel×dk,WK\ni∈Rdmodel×dk,WV\ni∈Rdmodel×dv\nandWO∈Rhdv×dmodel.\nInthisworkweemployh=8parallelattentionlayers,orheads.Foreachoftheseweuse\ndk=dv=dmodel/h=64.Duetothereduceddimensionofeachhead,thetotalcomputationalcost\nissimilartothatofsingle-headattentionwithfulldimensionality.\n3.2.3ApplicationsofAttentioninourModel\nTheTransformerusesmulti-headattentioninthreedifferentways:\n•In\"encoder-decoderattention\"layers,thequeriescomefromthepreviousdecoderlayer,\nandthememorykeysandvaluescomefromtheoutputoftheencoder.Thisallowsevery\npositioninthedecodertoattendoverallpositionsintheinputsequence.Thismimicsthe\ntypicalencoder-decoderattentionmechanismsinsequence-to-sequencemodelssuchas\n", "itioninthedecodertoattendoverallpositionsintheinputsequence.Thismimicsthe\ntypicalencoder-decoderattentionmechanismsinsequence-to-sequencemodelssuchas\n[38,2,9].•Theencodercontainsself-attentionlayers.Inaself-attentionlayerallofthekeys,values\nandqueriescomefromthesameplace,inthiscase,theoutputofthepreviouslayerinthe\nencoder.Eachpositionintheencodercanattendtoallpositionsinthepreviouslayerofthe\nencoder.\n•Similarly,self-attentionlayersinthedecoderalloweachpositioninthedecodertoattendto\nallpositionsinthedecoderuptoandincludingthatposition.Weneedtopreventleftward\ninformationflowinthedecodertopreservetheauto-regressiveproperty.Weimplementthis\ninsideofscaleddot-productattentionbymaskingout(settingto−∞)allvaluesintheinput\nofthesoftmaxwhichcorrespondtoillegalconnections.SeeFigure2.\n3.3Position-wiseFeed-ForwardNetworks\nInadditiontoattentionsub-layers,eachofthelayersinourencoderanddecodercontainsafully\nconnectedfeed-forwardnetwork,whichisappliedtoeachpositionseparatelyandidentically.This\nconsistsoftwolineartransformationswithaReLUactivationinbetween.\nFFN(x)=max(0,xW1+b1)W2+b2(2)\nWhilethelineartransformationsarethesameacrossdifferentpositions,theyusedifferentparameters\n", "withaReLUactivationinbetween.\nFFN(x)=max(0,xW1+b1)W2+b2(2)\nWhilethelineartransformationsarethesameacrossdifferentpositions,theyusedifferentparameters\nfromlayertolayer.Anotherwayofdescribingthisisastwoconvolutionswithkernelsize1.Thedimensionalityofinputandoutputisdmodel=512,andtheinner-layerhasdimensionality\ndff=2048.\n3.4EmbeddingsandSoftmax\nSimilarlytoothersequencetransductionmodels,weuselearnedembeddingstoconverttheinput\ntokensandoutputtokenstovectorsofdimensiondmodel.Wealsousetheusuallearnedlineartransfor-\nmationandsoftmaxfunctiontoconvertthedecoderoutputtopredictednext-tokenprobabilities.In\nourmodel,wesharethesameweightmatrixbetweenthetwoembeddinglayersandthepre-softmax\nlineartransformation,similarto[30].Intheembeddinglayers,wemultiplythoseweightsby√dmodel.\n5Table1:Maximumpathlengths,per-layercomplexityandminimumnumberofsequentialoperations\nfordifferentlayertypes.nisthesequencelength,distherepresentationdimension,kisthekernel\nsizeofconvolutionsandrthesizeoftheneighborhoodinrestrictedself-attention.\nLayerTypeComplexityperLayerSequentialMaximumPathLength\nOperations\nSelf-AttentionO(n2·d)O(1)O(1)\nRecurrentO(n·d2)O(n)O(n)\nConvolutionalO(k·n·d2)O(1)O(logk(n))\n", "eComplexityperLayerSequentialMaximumPathLength\nOperations\nSelf-AttentionO(n2·d)O(1)O(1)\nRecurrentO(n·d2)O(n)O(n)\nConvolutionalO(k·n·d2)O(1)O(logk(n))\nSelf-Attention(restricted)O(r·n·d)O(1)O(n/r)3.5PositionalEncoding\nSinceourmodelcontainsnorecurrenceandnoconvolution,inorderforthemodeltomakeuseofthe\norderofthesequence,wemustinjectsomeinformationabouttherelativeorabsolutepositionofthe\ntokensinthesequence.Tothisend,weadd\"positionalencodings\"totheinputembeddingsatthe\nbottomsoftheencoderanddecoderstacks.Thepositionalencodingshavethesamedimensiondmodel\nastheembeddings,sothatthetwocanbesummed.Therearemanychoicesofpositionalencodings,\nlearnedandfixed[9].\nInthiswork,weusesineandcosinefunctionsofdifferentfrequencies:\nPE(pos,2i)=sin(pos/100002i/dmodel)\nPE(pos,2i+1)=cos(pos/100002i/dmodel)\nwhereposisthepositionandiisthedimension.Thatis,eachdimensionofthepositionalencoding\ncorrespondstoasinusoid.Thewavelengthsformageometricprogressionfrom2πto10000·2π.We\nchosethisfunctionbecausewehypothesizeditwouldallowthemodeltoeasilylearntoattendby\nrelativepositions,sinceforanyfixedoffsetk,PEpos+kcanberepresentedasalinearfunctionof\n", "becausewehypothesizeditwouldallowthemodeltoeasilylearntoattendby\nrelativepositions,sinceforanyfixedoffsetk,PEpos+kcanberepresentedasalinearfunctionof\nPEpos.Wealsoexperimentedwithusinglearnedpositionalembeddings[9]instead,andfoundthatthetwo\nversionsproducednearlyidenticalresults(seeTable3row(E)).Wechosethesinusoidalversion\nbecauseitmayallowthemodeltoextrapolatetosequencelengthslongerthantheonesencountered\nduringtraining.\n4WhySelf-Attention\nInthissectionwecomparevariousaspectsofself-attentionlayerstotherecurrentandconvolu-\ntionallayerscommonlyusedformappingonevariable-lengthsequenceofsymbolrepresentations\n(x1,...,xn)toanothersequenceofequallength(z1,...,zn),withxi,zi∈Rd,suchasahidden\nlayerinatypicalsequencetransductionencoderordecoder.Motivatingouruseofself-attentionwe\nconsiderthreedesiderata.\nOneisthetotalcomputationalcomplexityperlayer.Anotheristheamountofcomputationthatcan\nbeparallelized,asmeasuredbytheminimumnumberofsequentialoperationsrequired.\nThethirdisthepathlengthbetweenlong-rangedependenciesinthenetwork.Learninglong-range\ndependenciesisakeychallengeinmanysequencetransductiontasks.Onekeyfactoraffectingthe\nabilitytolearnsuchdependenciesisthelengthofthepathsforwardandbackwardsignalshaveto\n", "ychallengeinmanysequencetransductiontasks.Onekeyfactoraffectingthe\nabilitytolearnsuchdependenciesisthelengthofthepathsforwardandbackwardsignalshaveto\ntraverseinthenetwork.Theshorterthesepathsbetweenanycombinationofpositionsintheinputandoutputsequences,theeasieritistolearnlong-rangedependencies[12].Hencewealsocompare\nthemaximumpathlengthbetweenanytwoinputandoutputpositionsinnetworkscomposedofthe\ndifferentlayertypes.\nAsnotedinTable1,aself-attentionlayerconnectsallpositionswithaconstantnumberofsequentially\nexecutedoperations,whereasarecurrentlayerrequiresO(n)sequentialoperations.Intermsof\ncomputationalcomplexity,self-attentionlayersarefasterthanrecurrentlayerswhenthesequence\n6lengthnissmallerthantherepresentationdimensionalityd,whichismostoftenthecasewith\nsentencerepresentationsusedbystate-of-the-artmodelsinmachinetranslations,suchasword-piece\n[38]andbyte-pair[31]representations.Toimprovecomputationalperformancefortasksinvolving\nverylongsequences,self-attentioncouldberestrictedtoconsideringonlyaneighborhoodofsizerin\ntheinputsequencecenteredaroundtherespectiveoutputposition.Thiswouldincreasethemaximum\npathlengthtoO(n/r).Weplantoinvestigatethisapproachfurtherinfuturework.\nAsingleconvolutionallayerwithkernelwidthk<ndoesnotconnectallpairsofinputandoutput\n", "hlengthtoO(n/r).Weplantoinvestigatethisapproachfurtherinfuturework.\nAsingleconvolutionallayerwithkernelwidthk<ndoesnotconnectallpairsofinputandoutput\npositions.DoingsorequiresastackofO(n/k)convolutionallayersinthecaseofcontiguouskernels,orO(logk(n))inthecaseofdilatedconvolutions[18],increasingthelengthofthelongestpaths\nbetweenanytwopositionsinthenetwork.Convolutionallayersaregenerallymoreexpensivethan\nrecurrentlayers,byafactorofk.Separableconvolutions[6],however,decreasethecomplexity\nconsiderably,toO(k·n·d+n·d2).Evenwithk=n,however,thecomplexityofaseparable\nconvolutionisequaltothecombinationofaself-attentionlayerandapoint-wisefeed-forwardlayer,\ntheapproachwetakeinourmodel.\nAssidebenefit,self-attentioncouldyieldmoreinterpretablemodels.Weinspectattentiondistributions\nfromourmodelsandpresentanddiscussexamplesintheappendix.Notonlydoindividualattention\nheadsclearlylearntoperformdifferenttasks,manyappeartoexhibitbehaviorrelatedtothesyntactic\nandsemanticstructureofthesentences.\n5Training\nThissectiondescribesthetrainingregimeforourmodels.\n5.1TrainingDataandBatching\nWetrainedonthestandardWMT2014English-Germandatasetconsistingofabout4.5million\n", "ctiondescribesthetrainingregimeforourmodels.\n5.1TrainingDataandBatching\nWetrainedonthestandardWMT2014English-Germandatasetconsistingofabout4.5million\nsentencepairs.Sentenceswereencodedusingbyte-pairencoding[3],whichhasasharedsource-targetvocabularyofabout37000tokens.ForEnglish-French,weusedthesignificantlylargerWMT\n2014English-Frenchdatasetconsistingof36Msentencesandsplittokensintoa32000word-piece\nvocabulary[38].Sentencepairswerebatchedtogetherbyapproximatesequencelength.Eachtraining\nbatchcontainedasetofsentencepairscontainingapproximately25000sourcetokensand25000\ntargettokens.\n5.2HardwareandSchedule\nWetrainedourmodelsononemachinewith8NVIDIAP100GPUs.Forourbasemodelsusing\nthehyperparametersdescribedthroughoutthepaper,eachtrainingsteptookabout0.4seconds.We\ntrainedthebasemodelsforatotalof100,000stepsor12hours.Forourbigmodels,(describedonthe\nbottomlineoftable3),steptimewas1.0seconds.Thebigmodelsweretrainedfor300,000steps\n(3.5days).\n5.3Optimizer\nWeusedtheAdamoptimizer[20]withβ1=0.9,β2=0.98andϵ=10−9.Wevariedthelearning\nrateoverthecourseoftraining,accordingtotheformula:\nlrate=d−0.5\n", "5.3Optimizer\nWeusedtheAdamoptimizer[20]withβ1=0.9,β2=0.98andϵ=10−9.Wevariedthelearning\nrateoverthecourseoftraining,accordingtotheformula:\nlrate=d−0.5\nmodel·min(step_num−0.5,step_num·warmup_steps−1.5)(3)Thiscorrespondstoincreasingthelearningratelinearlyforthefirstwarmup_stepstrainingsteps,\nanddecreasingitthereafterproportionallytotheinversesquarerootofthestepnumber.Weused\nwarmup_steps=4000.\n5.4Regularization\nWeemploythreetypesofregularizationduringtraining:\n7Table2:TheTransformerachievesbetterBLEUscoresthanpreviousstate-of-the-artmodelsonthe\nEnglish-to-GermanandEnglish-to-Frenchnewstest2014testsatafractionofthetrainingcost.\nModelBLEUTrainingCost(FLOPs)\nEN-DEEN-FREN-DEEN-FR\nByteNet[18]23.75\nDeep-Att+PosUnk[39]39.21.0·1020\nGNMT+RL[38]24.639.922.3·10191.4·1020\nConvS2S[9]25.1640.469.6·10181.5·1020\nMoE[32]26.0340.562.0·10191.2·1020\nDeep-Att+PosUnkEnsemble[39]40.48.0·1020\n", "0\nGNMT+RL[38]24.639.922.3·10191.4·1020\nConvS2S[9]25.1640.469.6·10181.5·1020\nMoE[32]26.0340.562.0·10191.2·1020\nDeep-Att+PosUnkEnsemble[39]40.48.0·1020\nGNMT+RLEnsemble[38]26.3041.161.8·10201.1·1021ConvS2SEnsemble[9]26.3641.297.7·10191.2·1021\nTransformer(basemodel)27.338.13.3·1018\nTransformer(big)28.441.82.3·1019\nResidualDropoutWeapplydropout[33]totheoutputofeachsub-layer,beforeitisaddedtothe\nsub-layerinputandnormalized.Inaddition,weapplydropouttothesumsoftheembeddingsandthe\npositionalencodingsinboththeencoderanddecoderstacks.Forthebasemodel,weusearateof\nPdrop=0.1.\nLabelSmoothingDuringtraining,weemployedlabelsmoothingofvalueϵls=0.1[36].This\nhurtsperplexity,asthemodellearnstobemoreunsure,butimprovesaccuracyandBLEUscore.\n6Results\n6.1MachineTranslation\nOntheWMT2014English-to-Germantranslationtask,thebigtransformermodel(Transformer(big)\ninTable2)outperformsthebestpreviouslyreportedmodels(includingensembles)bymorethan2.0\nBLEU,establishinganewstate-of-the-artBLEUscoreof28.4.Theconfigurationofthismodelis\n", "msthebestpreviouslyreportedmodels(includingensembles)bymorethan2.0\nBLEU,establishinganewstate-of-the-artBLEUscoreof28.4.Theconfigurationofthismodelis\nlistedinthebottomlineofTable3.Trainingtook3.5dayson8P100GPUs.Evenourbasemodelsurpassesallpreviouslypublishedmodelsandensembles,atafractionofthetrainingcostofanyof\nthecompetitivemodels.\nOntheWMT2014English-to-Frenchtranslationtask,ourbigmodelachievesaBLEUscoreof41.0,\noutperformingallofthepreviouslypublishedsinglemodels,atlessthan1/4thetrainingcostofthe\npreviousstate-of-the-artmodel.TheTransformer(big)modeltrainedforEnglish-to-Frenchused\ndropoutratePdrop=0.1,insteadof0.3.\nForthebasemodels,weusedasinglemodelobtainedbyaveragingthelast5checkpoints,which\nwerewrittenat10-minuteintervals.Forthebigmodels,weaveragedthelast20checkpoints.We\nusedbeamsearchwithabeamsizeof4andlengthpenaltyα=0.6[38].Thesehyperparameters\nwerechosenafterexperimentationonthedevelopmentset.Wesetthemaximumoutputlengthduring\ninferencetoinputlength+50,butterminateearlywhenpossible[38].\nTable2summarizesourresultsandcomparesourtranslationqualityandtrainingcoststoothermodel\n", "g\ninferencetoinputlength+50,butterminateearlywhenpossible[38].\nTable2summarizesourresultsandcomparesourtranslationqualityandtrainingcoststoothermodel\narchitecturesfromtheliterature.Weestimatethenumberoffloatingpointoperationsusedtotrainamodelbymultiplyingthetrainingtime,thenumberofGPUsused,andanestimateofthesustained\nsingle-precisionfloating-pointcapacityofeachGPU5.\n6.2ModelVariations\nToevaluatetheimportanceofdifferentcomponentsoftheTransformer,wevariedourbasemodel\nindifferentways,measuringthechangeinperformanceonEnglish-to-Germantranslationonthe\n5Weusedvaluesof2.8,3.7,6.0and9.5TFLOPSforK80,K40,M40andP100,respectively.\n8Table3:VariationsontheTransformerarchitecture.Unlistedvaluesareidenticaltothoseofthebase\nmodel.AllmetricsareontheEnglish-to-Germantranslationdevelopmentset,newstest2013.Listed\nperplexitiesareper-wordpiece,accordingtoourbyte-pairencoding,andshouldnotbecomparedto\nper-wordperplexities.\nNdmodeldffhdkdvPdropϵlstrainPPLBLEUparams\nsteps(dev)(dev)×106\nbase65122048864640.10.1100K4.9225.865\n(A)15125125.2924.9\n41281285.0025.5\n1632324.9125.8\n3216165.0125.4\n", "PdropϵlstrainPPLBLEUparams\nsteps(dev)(dev)×106\nbase65122048864640.10.1100K4.9225.865\n(A)15125125.2924.9\n41281285.0025.5\n1632324.9125.8\n3216165.0125.4\n(B)165.1625.158325.0125.460\n(C)26.1123.736\n45.1925.350\n84.8825.580\n25632325.7524.528\n10241281284.6626.0168\n10245.1225.453\n40964.7526.290\n(D)0.05.7724.6\n0.24.9525.5\n0.04.6725.3\n0.25.4725.7\n(E)positionalembeddinginsteadofsinusoids4.9225.7\nbig610244096160.3300K4.3326.4213\ndevelopmentset,newstest2013.Weusedbeamsearchasdescribedintheprevioussection,butno\ncheckpointaveraging.WepresenttheseresultsinTable3.\nInTable3rows(A),wevarythenumberofattentionheadsandtheattentionkeyandvaluedimensions,\nkeepingtheamountofcomputationconstant,asdescribedinSection3.2.2.Whilesingle-head\nattentionis0.9BLEUworsethanthebestsetting,qualityalsodropsoffwithtoomanyheads.\nInTable3rows(B),weobservethatreducingtheattentionkeysizedkhurtsmodelquality.This\nsuggeststhatdeterminingcompatibilityisnoteasyandthatamoresophisticatedcompatibility\n", ",weobservethatreducingtheattentionkeysizedkhurtsmodelquality.This\nsuggeststhatdeterminingcompatibilityisnoteasyandthatamoresophisticatedcompatibility\nfunctionthandotproductmaybebeneficial.Wefurtherobserveinrows(C)and(D)that,asexpected,biggermodelsarebetter,anddropoutisveryhelpfulinavoidingover-fitting.Inrow(E)wereplaceour\nsinusoidalpositionalencodingwithlearnedpositionalembeddings[9],andobservenearlyidentical\nresultstothebasemodel.\n6.3EnglishConstituencyParsing\nToevaluateiftheTransformercangeneralizetoothertasksweperformedexperimentsonEnglish\nconstituencyparsing.Thistaskpresentsspecificchallenges:theoutputissubjecttostrongstructural\nconstraintsandissignificantlylongerthantheinput.Furthermore,RNNsequence-to-sequence\nmodelshavenotbeenabletoattainstate-of-the-artresultsinsmall-dataregimes[37].\nWetraineda4-layertransformerwithdmodel=1024ontheWallStreetJournal(WSJ)portionofthe\nPennTreebank[25],about40Ktrainingsentences.Wealsotraineditinasemi-supervisedsetting,\nusingthelargerhigh-confidenceandBerkleyParsercorporafromwithapproximately17Msentences\n[37].Weusedavocabularyof16KtokensfortheWSJonlysettingandavocabularyof32Ktokens\nforthesemi-supervisedsetting.\n", "corporafromwithapproximately17Msentences\n[37].Weusedavocabularyof16KtokensfortheWSJonlysettingandavocabularyof32Ktokens\nforthesemi-supervisedsetting.\nWeperformedonlyasmallnumberofexperimentstoselectthedropout,bothattentionandresidual(section5.4),learningratesandbeamsizeontheSection22developmentset,allotherparameters\nremainedunchangedfromtheEnglish-to-Germanbasetranslationmodel.Duringinference,we\n9Table4:TheTransformergeneralizeswelltoEnglishconstituencyparsing(ResultsareonSection23\nofWSJ)\nParserTrainingWSJ23F1\nVinyals&Kaiserelal.(2014)[37]WSJonly,discriminative88.3\nPetrovetal.(2006)[29]WSJonly,discriminative90.4\nZhuetal.(2013)[40]WSJonly,discriminative90.4\nDyeretal.(2016)[8]WSJonly,discriminative91.7\nTransformer(4layers)WSJonly,discriminative91.3\nZhuetal.(2013)[40]semi-supervised91.3\nHuang&Harper(2009)[14]semi-supervised91.3\nMcCloskyetal.(2006)[26]semi-supervised92.1\nVinyals&Kaiserelal.(2014)[37]semi-supervised92.1\nTransformer(4layers)semi-supervised92.7\n", "mi-supervised91.3\nMcCloskyetal.(2006)[26]semi-supervised92.1\nVinyals&Kaiserelal.(2014)[37]semi-supervised92.1\nTransformer(4layers)semi-supervised92.7\nLuongetal.(2015)[23]multi-task93.0Dyeretal.(2016)[8]generative93.3\nincreasedthemaximumoutputlengthtoinputlength+300.Weusedabeamsizeof21andα=0.3\nforbothWSJonlyandthesemi-supervisedsetting.\nOurresultsinTable4showthatdespitethelackoftask-specifictuningourmodelperformssur-\nprisinglywell,yieldingbetterresultsthanallpreviouslyreportedmodelswiththeexceptionofthe\nRecurrentNeuralNetworkGrammar[8].\nIncontrasttoRNNsequence-to-sequencemodels[37],theTransformeroutperformstheBerkeley-\nParser[29]evenwhentrainingonlyontheWSJtrainingsetof40Ksentences.\n7Conclusion\nInthiswork,wepresentedtheTransformer,thefirstsequencetransductionmodelbasedentirelyon\nattention,replacingtherecurrentlayersmostcommonlyusedinencoder-decoderarchitectureswith\nmulti-headedself-attention.\nFortranslationtasks,theTransformercanbetrainedsignificantlyfasterthanarchitecturesbased\nonrecurrentorconvolutionallayers.OnbothWMT2014English-to-GermanandWMT2014\nEnglish-to-Frenchtranslationtasks,weachieveanewstateoftheart.Intheformertaskourbest\n", "entorconvolutionallayers.OnbothWMT2014English-to-GermanandWMT2014\nEnglish-to-Frenchtranslationtasks,weachieveanewstateoftheart.Intheformertaskourbest\nmodeloutperformsevenallpreviouslyreportedensembles.Weareexcitedaboutthefutureofattention-basedmodelsandplantoapplythemtoothertasks.We\nplantoextendtheTransformertoproblemsinvolvinginputandoutputmodalitiesotherthantextand\ntoinvestigatelocal,restrictedattentionmechanismstoefficientlyhandlelargeinputsandoutputs\nsuchasimages,audioandvideo.Makinggenerationlesssequentialisanotherresearchgoalsofours.\nThecodeweusedtotrainandevaluateourmodelsisavailableathttps://github.com/\ntensorflow/tensor2tensor.\nAcknowledgementsWearegratefultoNalKalchbrennerandStephanGouwsfortheirfruitful\ncomments,correctionsandinspiration.\nReferences\n[1]JimmyLeiBa,JamieRyanKiros,andGeoffreyEHinton.Layernormalization.arXivpreprint\narXiv:1607.06450,2016.\n[2]DzmitryBahdanau,KyunghyunCho,andYoshuaBengio.Neuralmachinetranslationbyjointly\nlearningtoalignandtranslate.CoRR,abs/1409.0473,2014.\n[3]DennyBritz,AnnaGoldie,Minh-ThangLuong,andQuocV.Le.Massiveexplorationofneural\n", "slationbyjointly\nlearningtoalignandtranslate.CoRR,abs/1409.0473,2014.\n[3]DennyBritz,AnnaGoldie,Minh-ThangLuong,andQuocV.Le.Massiveexplorationofneural\nmachinetranslationarchitectures.CoRR,abs/1703.03906,2017.[4]JianpengCheng,LiDong,andMirellaLapata.Longshort-termmemory-networksformachine\nreading.arXivpreprintarXiv:1601.06733,2016.\n10[5]KyunghyunCho,BartvanMerrienboer,CaglarGulcehre,FethiBougares,HolgerSchwenk,\nandYoshuaBengio.Learningphraserepresentationsusingrnnencoder-decoderforstatistical\nmachinetranslation.CoRR,abs/1406.1078,2014.\n[6]FrancoisChollet.Xception:Deeplearningwithdepthwiseseparableconvolutions.arXiv\npreprintarXiv:1610.02357,2016.\n[7]JunyoungChung,ÇaglarGülçehre,KyunghyunCho,andYoshuaBengio.Empiricalevaluation\nofgatedrecurrentneuralnetworksonsequencemodeling.CoRR,abs/1412.3555,2014.\n[8]ChrisDyer,AdhigunaKuncoro,MiguelBallesteros,andNoahA.Smith.Recurrentneural\n", "gatedrecurrentneuralnetworksonsequencemodeling.CoRR,abs/1412.3555,2014.\n[8]ChrisDyer,AdhigunaKuncoro,MiguelBallesteros,andNoahA.Smith.Recurrentneural\nnetworkgrammars.InProc.ofNAACL,2016.[9]JonasGehring,MichaelAuli,DavidGrangier,DenisYarats,andYannN.Dauphin.Convolu-\ntionalsequencetosequencelearning.arXivpreprintarXiv:1705.03122v2,2017.\n[10]AlexGraves.Generatingsequenceswithrecurrentneuralnetworks.arXivpreprint\narXiv:1308.0850,2013.\n[11]KaimingHe,XiangyuZhang,ShaoqingRen,andJianSun.Deepresiduallearningforim-\nagerecognition.InProceedingsoftheIEEEConferenceonComputerVisionandPattern\nRecognition,pages770–778,2016.\n[12]SeppHochreiter,YoshuaBengio,PaoloFrasconi,andJürgenSchmidhuber.Gradientflowin\nrecurrentnets:thedifficultyoflearninglong-termdependencies,2001.\n[13]SeppHochreiterandJürgenSchmidhuber.Longshort-termmemory.Neuralcomputation,\n9(8):1735–1780,1997.\n[14]ZhongqiangHuangandMaryHarper.Self-trainingPCFGgrammarswithlatentannotations\n", "midhuber.Longshort-termmemory.Neuralcomputation,\n9(8):1735–1780,1997.\n[14]ZhongqiangHuangandMaryHarper.Self-trainingPCFGgrammarswithlatentannotations\nacrosslanguages.InProceedingsofthe2009ConferenceonEmpiricalMethodsinNaturalLanguageProcessing,pages832–841.ACL,August2009.\n[15]RafalJozefowicz,OriolVinyals,MikeSchuster,NoamShazeer,andYonghuiWu.Exploring\nthelimitsoflanguagemodeling.arXivpreprintarXiv:1602.02410,2016.\n[16]ŁukaszKaiserandSamyBengio.Canactivememoryreplaceattention?InAdvancesinNeural\nInformationProcessingSystems,(NIPS),2016.\n[17]ŁukaszKaiserandIlyaSutskever.NeuralGPUslearnalgorithms.InInternationalConference\nonLearningRepresentations(ICLR),2016.\n[18]NalKalchbrenner,LasseEspeholt,KarenSimonyan,AaronvandenOord,AlexGraves,andKo-\nrayKavukcuoglu.Neuralmachinetranslationinlineartime.arXivpreprintarXiv:1610.10099v2,\n2017.\n[19]YoonKim,CarlDenton,LuongHoang,andAlexanderM.Rush.Structuredattentionnetworks.\nInInternationalConferenceonLearningRepresentations,2017.\n", "9v2,\n2017.\n[19]YoonKim,CarlDenton,LuongHoang,andAlexanderM.Rush.Structuredattentionnetworks.\nInInternationalConferenceonLearningRepresentations,2017.\n[20]DiederikKingmaandJimmyBa.Adam:Amethodforstochasticoptimization.InICLR,2015.[21]OleksiiKuchaievandBorisGinsburg.FactorizationtricksforLSTMnetworks.arXivpreprint\narXiv:1703.10722,2017.\n[22]ZhouhanLin,MinweiFeng,CiceroNogueiradosSantos,MoYu,BingXiang,Bowen\nZhou,andYoshuaBengio.Astructuredself-attentivesentenceembedding.arXivpreprint\narXiv:1703.03130,2017.\n[23]Minh-ThangLuong,QuocV.Le,IlyaSutskever,OriolVinyals,andLukaszKaiser.Multi-task\nsequencetosequencelearning.arXivpreprintarXiv:1511.06114,2015.\n[24]Minh-ThangLuong,HieuPham,andChristopherDManning.Effectiveapproachestoattention-\nbasedneuralmachinetranslation.arXivpreprintarXiv:1508.04025,2015.\n11[25]MitchellPMarcus,MaryAnnMarcinkiewicz,andBeatriceSantorini.Buildingalargeannotated\n", "dneuralmachinetranslation.arXivpreprintarXiv:1508.04025,2015.\n11[25]MitchellPMarcus,MaryAnnMarcinkiewicz,andBeatriceSantorini.Buildingalargeannotated\ncorpusofenglish:Thepenntreebank.Computationallinguistics,19(2):313–330,1993.[26]DavidMcClosky,EugeneCharniak,andMarkJohnson.Effectiveself-trainingforparsing.In\nProceedingsoftheHumanLanguageTechnologyConferenceoftheNAACL,MainConference,\npages152–159.ACL,June2006.\n[27]AnkurParikh,OscarTäckström,DipanjanDas,andJakobUszkoreit.Adecomposableattention\nmodel.InEmpiricalMethodsinNaturalLanguageProcessing,2016.\n[28]RomainPaulus,CaimingXiong,andRichardSocher.Adeepreinforcedmodelforabstractive\nsummarization.arXivpreprintarXiv:1705.04304,2017.\n[29]SlavPetrov,LeonBarrett,RomainThibaux,andDanKlein.Learningaccurate,compact,\nandinterpretabletreeannotation.InProceedingsofthe21stInternationalConferenceon\nComputationalLinguisticsand44thAnnualMeetingoftheACL,pages433–440.ACL,July\n2006.\n[30]OfirPressandLiorWolf.Usingtheoutputembeddingtoimprovelanguagemodels.arXiv\npreprintarXiv:1608.05859,2016.\n", "tingoftheACL,pages433–440.ACL,July\n2006.\n[30]OfirPressandLiorWolf.Usingtheoutputembeddingtoimprovelanguagemodels.arXiv\npreprintarXiv:1608.05859,2016.\n[31]RicoSennrich,BarryHaddow,andAlexandraBirch.Neuralmachinetranslationofrarewordswithsubwordunits.arXivpreprintarXiv:1508.07909,2015.\n[32]NoamShazeer,AzaliaMirhoseini,KrzysztofMaziarz,AndyDavis,QuocLe,GeoffreyHinton,\nandJeffDean.Outrageouslylargeneuralnetworks:Thesparsely-gatedmixture-of-experts\nlayer.arXivpreprintarXiv:1701.06538,2017.\n[33]NitishSrivastava,GeoffreyEHinton,AlexKrizhevsky,IlyaSutskever,andRuslanSalakhutdi-\nnov.Dropout:asimplewaytopreventneuralnetworksfromoverfitting.JournalofMachine\nLearningResearch,15(1):1929–1958,2014.\n[34]SainbayarSukhbaatar,ArthurSzlam,JasonWeston,andRobFergus.End-to-endmemory\nnetworks.InC.Cortes,N.D.Lawrence,D.D.Lee,M.Sugiyama,andR.Garnett,editors,\nAdvancesinNeuralInformationProcessingSystems28,pages2440–2448.CurranAssociates,\n", "orks.InC.Cortes,N.D.Lawrence,D.D.Lee,M.Sugiyama,andR.Garnett,editors,\nAdvancesinNeuralInformationProcessingSystems28,pages2440–2448.CurranAssociates,\nInc.,2015.[35]IlyaSutskever,OriolVinyals,andQuocVVLe.Sequencetosequencelearningwithneural\nnetworks.InAdvancesinNeuralInformationProcessingSystems,pages3104–3112,2014.\n[36]ChristianSzegedy,VincentVanhoucke,SergeyIoffe,JonathonShlens,andZbigniewWojna.\nRethinkingtheinceptionarchitectureforcomputervision.CoRR,abs/1512.00567,2015.\n[37]Vinyals&Kaiser,Koo,Petrov,Sutskever,andHinton.Grammarasaforeignlanguage.In\nAdvancesinNeuralInformationProcessingSystems,2015.\n[38]YonghuiWu,MikeSchuster,ZhifengChen,QuocVLe,MohammadNorouzi,Wolfgang\nMacherey,MaximKrikun,YuanCao,QinGao,KlausMacherey,etal.Google’sneuralmachine\ntranslationsystem:Bridgingthegapbetweenhumanandmachinetranslation.arXivpreprint\narXiv:1609.08144,2016.\n[39]JieZhou,YingCao,XuguangWang,PengLi,andWeiXu.Deeprecurrentmodelswith\n", "gthegapbetweenhumanandmachinetranslation.arXivpreprint\narXiv:1609.08144,2016.\n[39]JieZhou,YingCao,XuguangWang,PengLi,andWeiXu.Deeprecurrentmodelswith\nfast-forwardconnectionsforneuralmachinetranslation.CoRR,abs/1606.04199,2016.[40]MuhuaZhu,YueZhang,WenliangChen,MinZhang,andJingboZhu.Fastandaccurate\nshift-reduceconstituentparsing.InProceedingsofthe51stAnnualMeetingoftheACL(Volume\n1:LongPapers),pages434–443.ACL,August2013.\n12AttentionVisualizations\nInput-InputLayer5\nIt\nis\nin\nthis\nspirit\nthat\na\nmajority\nof\nAmerican\ngovernments\nhave\npassed\nnew\nlaws\nsince\n2009\nmaking\nthe\nregistration\nor\nvoting\nprocess\nmore\ndifficult\n.\n<EOS>\n<pad>\n<pad>\n<pad>\n<pad>\n<pad>\n<pad>\nIt\nis\nin\nthis\nspirit\nthat\na\nmajority\nof\nAmerican\ngovernments\nhave\npassed\nnew\nlaws\nsince\n2009\nmaking\nthe\nregistration\nor\nvoting\nprocess\nmore\ndifficult\n.\n<EOS>\n<pad>\n<pad>\n<pad>\n<pad>\n<pad>\n<pad>\n", "ican\ngovernments\nhave\npassed\nnew\nlaws\nsince\n2009\nmaking\nthe\nregistration\nor\nvoting\nprocess\nmore\ndifficult\n.\n<EOS>\n<pad>\n<pad>\n<pad>\n<pad>\n<pad>\n<pad>\nFigure3:Anexampleoftheattentionmechanismfollowinglong-distancedependenciesintheencoderself-attentioninlayer5of6.Manyoftheattentionheadsattendtoadistantdependencyof\ntheverb‘making’,completingthephrase‘making...moredifficult’.Attentionshereshownonlyfor\ntheword‘making’.Differentcolorsrepresentdifferentheads.Bestviewedincolor.\n13Input-InputLayer5\nThe\nLaw\nwill\nnever\nbe\nperfect\n,\nbut\nits\napplication\nshould\nbe\njust\n-\nthis\nis\nwhat\nwe\nare\nmissing\n,\nin\nmy\nopinion\n.\n<EOS>\n<pad>\nThe\nLaw\nwill\nnever\nbe\nperfect\n,\nbut\nits\napplication\nshould\nbe\njust\n-\nthis\nis\nwhat\nwe\nare\nmissing\n,\nin\nmy\nopinion\n.\n<EOS>\n<pad>\nInput-InputLayer5\nThe\nLaw\nwill\nnever\nbe\nperfect\n,\nbut\nits\napplication\nshould\nbe\njust\n-\nthis\nis\nwhat\nwe\nare\nmissing\n,\nin\nmy\nopinion\n.\n<EOS>\n<pad>\nThe\nLaw\nwill\nnever\nbe\nperfect\n,\nbut\nits\napplication\nshould\nbe\n", "\napplication\nshould\nbe\njust\n-\nthis\nis\nwhat\nwe\nare\nmissing\n,\nin\nmy\nopinion\n.\n<EOS>\n<pad>\nThe\nLaw\nwill\nnever\nbe\nperfect\n,\nbut\nits\napplication\nshould\nbe\njust-\nthis\nis\nwhat\nwe\nare\nmissing\n,\nin\nmy\nopinion\n.\n<EOS>\n<pad>Figure4:Twoattentionheads,alsoinlayer5of6,apparentlyinvolvedinanaphoraresolution.Top:\nFullattentionsforhead5.Bottom:Isolatedattentionsfromjusttheword‘its’forattentionheads5\nand6.Notethattheattentionsareverysharpforthisword.\n14Input-InputLayer5\nThe\nLaw\nwill\nnever\nbe\nperfect\n,\nbut\nits\napplication\nshould\nbe\njust\n-\nthis\nis\nwhat\nwe\nare\nmissing\n,\nin\nmy\nopinion\n.\n<EOS>\n<pad>\nThe\nLaw\nwill\nnever\nbe\nperfect\n,\nbut\nits\napplication\nshould\nbe\njust\n-\nthis\nis\nwhat\nwe\nare\nmissing\n,\nin\nmy\nopinion\n.\n<EOS>\n<pad>\nInput-InputLayer5\nThe\nLaw\nwill\nnever\nbe\nperfect\n,\nbut\nits\napplication\nshould\nbe\njust\n-\nthis\nis\nwhat\nwe\nare\nmissing\n,\nin\nmy\nopinion\n.\n<EOS>\n<pad>\nThe\nLaw\nwill\nnever\nbe\nperfect\n", "\nwill\nnever\nbe\nperfect\n,\nbut\nits\napplication\nshould\nbe\njust\n-\nthis\nis\nwhat\nwe\nare\nmissing\n,\nin\nmy\nopinion\n.\n<EOS>\n<pad>\nThe\nLaw\nwill\nnever\nbe\nperfect\n,but\nits\napplication\nshould\nbe\njust\n-\nthis\nis\nwhat\nwe\nare\nmissing\n,\nin\nmy\nopinion\n.\n<EOS>\n<pad>Figure5:Manyoftheattentionheadsexhibitbehaviourthatseemsrelatedtothestructureofthe\nsentence.Wegivetwosuchexamplesabove,fromtwodifferentheadsfromtheencoderself-attention\natlayer5of6.Theheadsclearlylearnedtoperformdifferenttasks.\n15\n"]